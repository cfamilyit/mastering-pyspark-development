{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7f1c5c",
   "metadata": {},
   "source": [
    "# <center> PySpark Core Programming - RDD Programming </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7208fe",
   "metadata": {},
   "source": [
    "### PySpark Core Program in Interactive Mode using SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda1a6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n",
      "<class 'pyspark.context.SparkContext'>\n"
     ]
    }
   ],
   "source": [
    "print(sc)\n",
    "print(type(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815545d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of Rows of an RDD: 4\n",
      "******************************\n",
      "All rows of an RDD:\n",
      "====================\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n",
      "hadoop hdfs hadoop hdfs\n",
      "hadoop hdfs hive spark\n",
      "Few rows of an RDD:\n",
      "====================\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.textFile(\"C:\\\\Cfamily IT\\\\Data\\\\comment.txt\")\n",
    "\n",
    "print(\"No.of Rows of an RDD:\",rdd1.count())\n",
    "print(\"*\"*30)\n",
    "print(\"All rows of an RDD:\")\n",
    "print(\"====================\")\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "\n",
    "print(\"Few rows of an RDD:\")\n",
    "print(\"====================\")\n",
    "for row in rdd1.take(2):\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551541ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8ee4ce1",
   "metadata": {},
   "source": [
    "### PySpark Core Program in In teractive Mode using SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e27cad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000022F37CA07C0>\n",
      "<class 'pyspark.sql.session.SparkSession'>\n"
     ]
    }
   ],
   "source": [
    "print(spark)\n",
    "print(type(spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38e72c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of Rows of an RDD: 4\n",
      "******************************\n",
      "All rows of an RDD:\n",
      "====================\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n",
      "hadoop hdfs hadoop hdfs\n",
      "hadoop hdfs hive spark\n",
      "Few rows of an RDD:\n",
      "====================\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/comment.txt\")\n",
    "\n",
    "print(\"No.of Rows of an RDD:\",rdd1.count())\n",
    "print(\"*\"*30)\n",
    "print(\"All rows of an RDD:\")\n",
    "print(\"====================\")\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "\n",
    "print(\"Few rows of an RDD:\")\n",
    "print(\"====================\")\n",
    "for row in rdd1.take(2):\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec0b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f33f67cb",
   "metadata": {},
   "source": [
    "## How to create an RDD?\n",
    "### 1.Create an RDD by loading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee91757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count no.of rows of an RDD: 4\n",
      "******************************\n",
      "All rows of an RDD:\n",
      "******************************\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n",
      "hadoop hdfs hadoop hdfs\n",
      "hadoop hdfs hive spark\n",
      "Few rows of an RDD:\n",
      "******************************\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/comment.txt\")\n",
    "print(\"Count no.of rows of an RDD:\",rdd1.count())\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"All rows of an RDD:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "\n",
    "print(\"Few rows of an RDD:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd1.take(2):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e704a6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4547d4ff",
   "metadata": {},
   "source": [
    "### Create an RDD by converting Local Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a92ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count no.of rows of an RDD: 5\n",
      "******************************\n",
      "All rows of an RDD:\n",
      "******************************\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "******************************\n",
      "Few rows of an RDD:\n",
      "******************************\n",
      "10\n",
      "20\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "li = [10,20,30,40,50]\n",
    "rdd1 = spark.sparkContext.parallelize(li)\n",
    "print(\"Count no.of rows of an RDD:\",rdd1.count())\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"All rows of an RDD:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "print(\"Few rows of an RDD:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd1.take(2):\n",
    "    print(row)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664ce3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f162c9af",
   "metadata": {},
   "source": [
    "### Create an RDD by applying transformation on existing RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b8f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Transformation All rows of an RDD:\n",
      "******************************\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "******************************\n",
      "After Transformation All rows of an RDD:\n",
      "******************************\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n",
      "51\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "li = [10,20,30,40,50]\n",
    "rdd1 = spark.sparkContext.parallelize(li)\n",
    "print(\"Before Transformation All rows of an RDD:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "rdd2 = rdd1.map(lambda x:x+1)\n",
    "print(\"After Transformation All rows of an RDD:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd2.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968399d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6e651b4",
   "metadata": {},
   "source": [
    "### map() Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "864fc97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before map() Transformation:\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "******************************\n",
      "After map() Transformation:\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n",
      "51\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "li = [10,20,30,40,50]\n",
    "rdd1 = spark.sparkContext.parallelize(li)\n",
    "print(\"Before map() Transformation:\")\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "rdd2 = rdd1.map(lambda x:x+1)\n",
    "print(\"After map() Transformation:\")\n",
    "for row in rdd2.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a50ab57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63ee8ebb",
   "metadata": {},
   "source": [
    "### flatMap() Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bfad10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before flatMap() Transformation:\n",
      "******************************\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n",
      "hadoop hdfs hadoop hdfs\n",
      "hadoop hdfs hive spark\n",
      "******************************\n",
      "After flatMap() Transformation:\n",
      "******************************\n",
      "spark\n",
      "hadoop\n",
      "spark\n",
      "hive\n",
      "spark\n",
      "spark\n",
      "hive\n",
      "hive\n",
      "hadoop\n",
      "hdfs\n",
      "hadoop\n",
      "hdfs\n",
      "hadoop\n",
      "hdfs\n",
      "hive\n",
      "spark\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/comment.txt\")\n",
    "print(\"Before flatMap() Transformation:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "rdd2 = rdd1.flatMap(lambda x:x.split(\" \"))\n",
    "print(\"After flatMap() Transformation:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd2.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4d7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba3b6ab6",
   "metadata": {},
   "source": [
    "### filter() Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "556c41cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filter() Transformation:\n",
      "******************************\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n",
      "hadoop hdfs hadoop hdfs\n",
      "hadoop hdfs hive spark\n",
      "******************************\n",
      "After filter() Transformation:\n",
      "******************************\n",
      "spark hadoop spark hive\n",
      "hadoop hdfs hadoop hdfs\n",
      "hadoop hdfs hive spark\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/comment.txt\")\n",
    "print(\"Before filter() Transformation:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "rdd2 = rdd1.filter(lambda x:\"hadoop\" in x)\n",
    "print(\"After filter() Transformation:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd2.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c089c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33ce61e5",
   "metadata": {},
   "source": [
    "### reduceByKey() transformation\n",
    "### E.g:- Write PySpark RDD Program to find Word Count (Each Word repeated how many times?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dbaff4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark hadoop spark hive\n",
      "spark spark hive hive\n",
      "hadoop hdfs hadoop hdfs\n",
      "hadoop hdfs hive spark\n",
      "******************************\n",
      "spark\n",
      "hadoop\n",
      "spark\n",
      "hive\n",
      "spark\n",
      "spark\n",
      "hive\n",
      "hive\n",
      "hadoop\n",
      "hdfs\n",
      "hadoop\n",
      "hdfs\n",
      "hadoop\n",
      "hdfs\n",
      "hive\n",
      "spark\n",
      "******************************\n",
      "('spark', 1)\n",
      "('hadoop', 1)\n",
      "('spark', 1)\n",
      "('hive', 1)\n",
      "('spark', 1)\n",
      "('spark', 1)\n",
      "('hive', 1)\n",
      "('hive', 1)\n",
      "('hadoop', 1)\n",
      "('hdfs', 1)\n",
      "('hadoop', 1)\n",
      "('hdfs', 1)\n",
      "('hadoop', 1)\n",
      "('hdfs', 1)\n",
      "('hive', 1)\n",
      "('spark', 1)\n",
      "******************************\n",
      "('hadoop', 4)\n",
      "('hive', 4)\n",
      "('hdfs', 3)\n",
      "('spark', 5)\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/comment.txt\")\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "rdd2 = rdd1.flatMap(lambda x:x.split(\" \"))\n",
    "for row in rdd2.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "rdd3 = rdd2.map(lambda x:(x,1))\n",
    "for row in rdd3.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "rdd4 = rdd3.reduceByKey(lambda x,y:x+y)\n",
    "for row in rdd4.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5bdfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hadoop', 4)\n",
      "('hive', 4)\n",
      "('hdfs', 3)\n",
      "('spark', 5)\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "wordCountRDD = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/comment.txt\").flatMap(lambda x:x.split(\" \")).map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)\n",
    "for row in wordCountRDD.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd55cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "454fa79f",
   "metadata": {},
   "source": [
    "## Partitions in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffa4c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of Partitions of an RDD: 2\n"
     ]
    }
   ],
   "source": [
    "# small file -- Local File\n",
    "rdd1 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/comment.txt\")\n",
    "print(\"No.of Partitions of an RDD:\",rdd1.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8adaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04fc65e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of Partitions of an RDD: 27\n"
     ]
    }
   ],
   "source": [
    "# big file -- Local File\n",
    "rdd2 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/bigfile.csv\")\n",
    "print(\"No.of Partitions of an RDD:\",rdd2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f5ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd0496d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of Partitions of an RDD: 5\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/comment.txt\",4)\n",
    "print(\"No.of Partitions of an RDD:\",rdd1.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e0ce35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a277e0fe",
   "metadata": {},
   "source": [
    "### repartition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1749f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default No.of Partitions of an RDD: 2\n",
      "Increased No.of Partitions of an RDD: 10\n",
      "Decreased No.of Partitions of an RDD: 5\n"
     ]
    }
   ],
   "source": [
    "#default Partitions\n",
    "rdd1 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/comment.txt\")\n",
    "print(\"Default No.of Partitions of an RDD:\",rdd1.getNumPartitions())\n",
    "\n",
    "#Increase Partitions\n",
    "rdd2 = rdd1.repartition(10)\n",
    "print(\"Increased No.of Partitions of an RDD:\",rdd2.getNumPartitions())\n",
    "\n",
    "#Decrease Partitions\n",
    "rdd3 = rdd2.repartition(5)\n",
    "print(\"Decreased No.of Partitions of an RDD:\",rdd3.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74090aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64f6acb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default No.of Partitions of an RDD: 2\n",
      "Increased No.of Partitions of an RDD: 10\n",
      "Decreased No.of Partitions of an RDD: 5\n"
     ]
    }
   ],
   "source": [
    "#default Partitions\n",
    "rdd1 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/comment.txt\")\n",
    "print(\"Default No.of Partitions of an RDD:\",rdd1.getNumPartitions())\n",
    "\n",
    "#Increase Partitions\n",
    "rdd2 = rdd1.repartition(10)\n",
    "print(\"Increased No.of Partitions of an RDD:\",rdd2.getNumPartitions())\n",
    "\n",
    "#Decrease Partitions\n",
    "rdd3 = rdd2.coalesce(5)\n",
    "print(\"Decreased No.of Partitions of an RDD:\",rdd3.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73518e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ef6ca08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default No.of Partitions of an RDD: 2\n",
      "Increased No.of Partitions of an RDD: 2\n",
      "Decreased No.of Partitions of an RDD: 1\n"
     ]
    }
   ],
   "source": [
    "#default Partitions\n",
    "rdd1 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/comment.txt\")\n",
    "print(\"Default No.of Partitions of an RDD:\",rdd1.getNumPartitions())\n",
    "\n",
    "#Increase Partitions\n",
    "rdd2 = rdd1.coalesce(10)\n",
    "print(\"Increased No.of Partitions of an RDD:\",rdd2.getNumPartitions())\n",
    "#Decrease Partitions\n",
    "rdd3 = rdd2.coalesce(1)\n",
    "print(\"Decreased No.of Partitions of an RDD:\",rdd3.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f07942b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b759874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6df6595",
   "metadata": {},
   "source": [
    "## <center> Persistence </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792ce089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C:/Cfamily IT/Data/bigfile.csv MapPartitionsRDD[8] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "## Step1 : Create an RDD\n",
    "rdd1 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/bigfile.csv\")\n",
    "\n",
    "## Verify an RDD Peristed or Not\n",
    "## got to Spark Web UI and Check\n",
    "\n",
    "### Upto now there are no RDD's persisted.\n",
    "\n",
    "## Step3: Persist an RDD\n",
    "rdd1.persist(StorageLevel.MEMORY_ONLY) # Here we may going to gerror saying \"NameError: name 'StorageLevel' is not defined\"\n",
    "\n",
    "### How to resolve -- We  need to import \"SorageLevel\" class\n",
    "\n",
    "## Verify an RDD Peristed or Not in Spark Web UI\n",
    "\n",
    "### Still We are not seeing -- Why\n",
    "### RDDs are Lazily Evaluated -- Until we perform an Action it will not persist\n",
    "rdd1.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da9ff6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C:/Cfamily IT/Data/bigfile.csv MapPartitionsRDD[8] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step4 : un-persist an RDD\n",
    "rdd1.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0eec53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10596993"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step5: Persist an RDD\n",
    "rdd1.persist(StorageLevel.MEMORY_ONLY_2)\n",
    "rdd1.count()\n",
    "## Verify an RDD Peristed or Not in Spark Web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8906bbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C:/Cfamily IT/Data/bigfile.csv MapPartitionsRDD[8] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step6 : un-persist an RDD\n",
    "rdd1.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db814e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b25dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10596993"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step5: Persist an RDD\n",
    "rdd1.persist(StorageLevel.DISK_ONLY)\n",
    "rdd1.count()\n",
    "## Verify an RDD Peristed or Not in Spark Web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28cf29c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C:/Cfamily IT/Data/bigfile.csv MapPartitionsRDD[8] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step8 : un-persist an RDD\n",
    "rdd1.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bbe0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a190f440",
   "metadata": {},
   "source": [
    "## <center> Set Operators or Operations in PySpark </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f828d9ad",
   "metadata": {},
   "source": [
    "### Union Set Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcb8b145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "No.of Records of calllogRDD: 206\n",
      "No.of Partitions of calllogRDD: 2\n",
      "No.of Records of successRDD: 155\n",
      "No.of default Partitions of successRDD: 2\n",
      "No.of Modified Partitions of successRDD: 10\n",
      "No.of Records of droppedRDD: 28\n",
      "No.of Partitions of droppedRDD: 2\n",
      "No.of Modified Partitions of droppedRDD: 8\n",
      "No.of Records of failedRDD: 23\n",
      "No.of Partitions of failedRDD: 2\n",
      "No.of Modified Partitions of failedRDD: 6\n",
      "******************************\n",
      "UNION Operation:\n",
      "******************************\n",
      "No.of Records of unionRDD: 361\n",
      "No.of Partitions of unionRDD: 12\n"
     ]
    }
   ],
   "source": [
    "calllogRDD = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/calllogdata\")\n",
    "print(\"*\"*30)\n",
    "print(\"No.of Records of calllogRDD:\",calllogRDD.count())\n",
    "print(\"No.of Partitions of calllogRDD:\",calllogRDD.getNumPartitions())\n",
    "\n",
    "successRDD = calllogRDD.filter(lambda x:\"SUCCESS\" in x)\n",
    "print(\"No.of Records of successRDD:\",successRDD.count())\n",
    "print(\"No.of default Partitions of successRDD:\",successRDD.getNumPartitions())\n",
    "successRDD = successRDD.repartition(10)\n",
    "print(\"No.of Modified Partitions of successRDD:\",successRDD.getNumPartitions())\n",
    "\n",
    "droppedRDD = calllogRDD.filter(lambda x:\"DROPPED\" in x)\n",
    "print(\"No.of Records of droppedRDD:\",droppedRDD.count())\n",
    "print(\"No.of Partitions of droppedRDD:\",droppedRDD.getNumPartitions())\n",
    "droppedRDD = droppedRDD.repartition(8)\n",
    "print(\"No.of Modified Partitions of droppedRDD:\",droppedRDD.getNumPartitions())\n",
    "\n",
    "\n",
    "failedRDD = calllogRDD.filter(lambda x:\"FAILED\" in x)\n",
    "print(\"No.of Records of failedRDD:\",failedRDD.count())\n",
    "print(\"No.of Partitions of failedRDD:\",failedRDD.getNumPartitions())\n",
    "failedRDD = failedRDD.repartition(6)\n",
    "print(\"No.of Modified Partitions of failedRDD:\",failedRDD.getNumPartitions())\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"UNION Operation:\")\n",
    "print(\"*\"*30)\n",
    "unionRDD = calllogRDD.union(successRDD) #206+155 =>361 (records), 2+10=>12 partitions(assume)\n",
    "print(\"No.of Records of unionRDD:\",unionRDD.count())\n",
    "print(\"No.of Partitions of unionRDD:\",unionRDD.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477bcb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f401ebd",
   "metadata": {},
   "source": [
    "### Subtraction Operator in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "912b8c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "No.of Records of calllogRDD: 206\n",
      "No.of Partitions of calllogRDD: 2\n",
      "No.of Records of successRDD: 155\n",
      "No.of default Partitions of successRDD: 2\n",
      "No.of Modified Partitions of successRDD: 10\n",
      "No.of Records of droppedRDD: 28\n",
      "No.of Partitions of droppedRDD: 2\n",
      "No.of Modified Partitions of droppedRDD: 8\n",
      "No.of Records of failedRDD: 23\n",
      "No.of Partitions of failedRDD: 2\n",
      "No.of Modified Partitions of failedRDD: 6\n",
      "******************************\n",
      "Subtraction Operation:\n",
      "******************************\n",
      "No.of Records of subtractionRDD: 51\n",
      "No.of Partitions of subtractionRDD: 12\n"
     ]
    }
   ],
   "source": [
    "calllogRDD = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/calllogdata\")\n",
    "print(\"*\"*30)\n",
    "print(\"No.of Records of calllogRDD:\",calllogRDD.count())\n",
    "print(\"No.of Partitions of calllogRDD:\",calllogRDD.getNumPartitions())\n",
    "\n",
    "successRDD = calllogRDD.filter(lambda x:\"SUCCESS\" in x)\n",
    "print(\"No.of Records of successRDD:\",successRDD.count())\n",
    "print(\"No.of default Partitions of successRDD:\",successRDD.getNumPartitions())\n",
    "successRDD = successRDD.repartition(10)\n",
    "print(\"No.of Modified Partitions of successRDD:\",successRDD.getNumPartitions())\n",
    "\n",
    "droppedRDD = calllogRDD.filter(lambda x:\"DROPPED\" in x)\n",
    "print(\"No.of Records of droppedRDD:\",droppedRDD.count())\n",
    "print(\"No.of Partitions of droppedRDD:\",droppedRDD.getNumPartitions())\n",
    "droppedRDD = droppedRDD.repartition(8)\n",
    "print(\"No.of Modified Partitions of droppedRDD:\",droppedRDD.getNumPartitions())\n",
    "\n",
    "\n",
    "failedRDD = calllogRDD.filter(lambda x:\"FAILED\" in x)\n",
    "print(\"No.of Records of failedRDD:\",failedRDD.count())\n",
    "print(\"No.of Partitions of failedRDD:\",failedRDD.getNumPartitions())\n",
    "failedRDD = failedRDD.repartition(6)\n",
    "print(\"No.of Modified Partitions of failedRDD:\",failedRDD.getNumPartitions())\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"Subtraction Operation:\")\n",
    "print(\"*\"*30)\n",
    "subtractionRDD = calllogRDD.subtract(successRDD) #206-155 =>51 (records), 2+10=>12 partitions(assume)\n",
    "print(\"No.of Records of subtractionRDD:\",subtractionRDD.count())\n",
    "print(\"No.of Partitions of subtractionRDD:\",subtractionRDD.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf7a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e835d64",
   "metadata": {},
   "source": [
    "### Intersection Operator in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f3cf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "No.of Records of calllogRDD: 206\n",
      "No.of Partitions of calllogRDD: 2\n",
      "No.of Records of successRDD: 155\n",
      "No.of default Partitions of successRDD: 2\n",
      "No.of Modified Partitions of successRDD: 10\n",
      "No.of Records of droppedRDD: 28\n",
      "No.of Partitions of droppedRDD: 2\n",
      "No.of Modified Partitions of droppedRDD: 8\n",
      "No.of Records of failedRDD: 23\n",
      "No.of Partitions of failedRDD: 2\n",
      "No.of Modified Partitions of failedRDD: 6\n",
      "******************************\n",
      "Intersection Operation:\n",
      "******************************\n",
      "No.of Records of intersectionRDD: 155\n",
      "No.of Partitions of intersectionRDD: 12\n"
     ]
    }
   ],
   "source": [
    "calllogRDD = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/calllogdata\")\n",
    "print(\"*\"*30)\n",
    "print(\"No.of Records of calllogRDD:\",calllogRDD.count())\n",
    "print(\"No.of Partitions of calllogRDD:\",calllogRDD.getNumPartitions())\n",
    "\n",
    "successRDD = calllogRDD.filter(lambda x:\"SUCCESS\" in x)\n",
    "print(\"No.of Records of successRDD:\",successRDD.count())\n",
    "print(\"No.of default Partitions of successRDD:\",successRDD.getNumPartitions())\n",
    "successRDD = successRDD.repartition(10)\n",
    "print(\"No.of Modified Partitions of successRDD:\",successRDD.getNumPartitions())\n",
    "\n",
    "droppedRDD = calllogRDD.filter(lambda x:\"DROPPED\" in x)\n",
    "print(\"No.of Records of droppedRDD:\",droppedRDD.count())\n",
    "print(\"No.of Partitions of droppedRDD:\",droppedRDD.getNumPartitions())\n",
    "droppedRDD = droppedRDD.repartition(8)\n",
    "print(\"No.of Modified Partitions of droppedRDD:\",droppedRDD.getNumPartitions())\n",
    "\n",
    "\n",
    "failedRDD = calllogRDD.filter(lambda x:\"FAILED\" in x)\n",
    "print(\"No.of Records of failedRDD:\",failedRDD.count())\n",
    "print(\"No.of Partitions of failedRDD:\",failedRDD.getNumPartitions())\n",
    "failedRDD = failedRDD.repartition(6)\n",
    "print(\"No.of Modified Partitions of failedRDD:\",failedRDD.getNumPartitions())\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"Intersection Operation:\")\n",
    "print(\"*\"*30)\n",
    "intersectionRDD = calllogRDD.intersection(successRDD) #206-155 =>155 (records), 2+10=>12 partitions(assume)\n",
    "print(\"No.of Records of intersectionRDD:\",intersectionRDD.count())\n",
    "print(\"No.of Partitions of intersectionRDD:\",intersectionRDD.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfa6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c510139",
   "metadata": {},
   "source": [
    "### Cartesian Operator in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c72b90ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "No.of Records of calllogRDD: 206\n",
      "No.of Partitions of calllogRDD: 2\n",
      "No.of Records of successRDD: 155\n",
      "No.of default Partitions of successRDD: 2\n",
      "No.of Modified Partitions of successRDD: 10\n",
      "No.of Records of droppedRDD: 28\n",
      "No.of Partitions of droppedRDD: 2\n",
      "No.of Modified Partitions of droppedRDD: 8\n",
      "No.of Records of failedRDD: 23\n",
      "No.of Partitions of failedRDD: 2\n",
      "No.of Modified Partitions of failedRDD: 6\n",
      "******************************\n",
      "Cartesian Operation:\n",
      "******************************\n",
      "No.of Records of cartesianRDD: 31930\n",
      "No.of Partitions of cartesianRDD: 20\n"
     ]
    }
   ],
   "source": [
    "calllogRDD = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/calllogdata\")\n",
    "print(\"*\"*30)\n",
    "print(\"No.of Records of calllogRDD:\",calllogRDD.count())\n",
    "print(\"No.of Partitions of calllogRDD:\",calllogRDD.getNumPartitions())\n",
    "\n",
    "successRDD = calllogRDD.filter(lambda x:\"SUCCESS\" in x)\n",
    "print(\"No.of Records of successRDD:\",successRDD.count())\n",
    "print(\"No.of default Partitions of successRDD:\",successRDD.getNumPartitions())\n",
    "successRDD = successRDD.repartition(10)\n",
    "print(\"No.of Modified Partitions of successRDD:\",successRDD.getNumPartitions())\n",
    "\n",
    "droppedRDD = calllogRDD.filter(lambda x:\"DROPPED\" in x)\n",
    "print(\"No.of Records of droppedRDD:\",droppedRDD.count())\n",
    "print(\"No.of Partitions of droppedRDD:\",droppedRDD.getNumPartitions())\n",
    "droppedRDD = droppedRDD.repartition(8)\n",
    "print(\"No.of Modified Partitions of droppedRDD:\",droppedRDD.getNumPartitions())\n",
    "\n",
    "\n",
    "failedRDD = calllogRDD.filter(lambda x:\"FAILED\" in x)\n",
    "print(\"No.of Records of failedRDD:\",failedRDD.count())\n",
    "print(\"No.of Partitions of failedRDD:\",failedRDD.getNumPartitions())\n",
    "failedRDD = failedRDD.repartition(6)\n",
    "print(\"No.of Modified Partitions of failedRDD:\",failedRDD.getNumPartitions())\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"Cartesian Operation:\")\n",
    "print(\"*\"*30)\n",
    "cartesianRDD = calllogRDD.cartesian(successRDD) #206-155 =>155 (records), 2*10=>20 partitions(assume)\n",
    "print(\"No.of Records of cartesianRDD:\",cartesianRDD.count())\n",
    "print(\"No.of Partitions of cartesianRDD:\",cartesianRDD.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5a802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02161529",
   "metadata": {},
   "source": [
    "## <center> Aggregation Operators in PySpark </center>\n",
    "### reduce aggregation operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb68740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "********************\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "li = [1,2,3,4,5,6,7,8,9]\n",
    "rdd1 = spark.sparkContext.parallelize(li,2)\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*20)\n",
    "\n",
    "res = rdd1.reduce(lambda x,y:x+y)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0554239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b357a47",
   "metadata": {},
   "source": [
    "### fold aggregation operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "060dcb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "********************\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "li = [1,2,3,4,5,6,7,8,9]\n",
    "rdd1 = spark.sparkContext.parallelize(li,2)\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*20)\n",
    "\n",
    "res = rdd1.fold(2,lambda x,y:x+y)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c87420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40e77230",
   "metadata": {},
   "source": [
    "### aggregate aggregation operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "446e8945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "********************\n",
      "888\n"
     ]
    }
   ],
   "source": [
    "li = [1,2,3,4,5,6,7,8,9]\n",
    "rdd1 = spark.sparkContext.parallelize(li,2)\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*20)\n",
    "\n",
    "res = rdd1.aggregate(2,lambda x,y:(x+y),lambda a,b:(a*b))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68395be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2513679d",
   "metadata": {},
   "source": [
    "## <center> Pair RDD in PySpark </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62007981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Key,Value Pair:\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n",
      "hadoop hdfs hadoop hdfs\n",
      "hadoop hdfs hive spark\n",
      "******************************\n",
      "After Key,Value Pair:\n",
      "('spark', 1)\n",
      "('hadoop', 1)\n",
      "('spark', 1)\n",
      "('hive', 1)\n",
      "('spark', 1)\n",
      "('spark', 1)\n",
      "('hive', 1)\n",
      "('hive', 1)\n",
      "('hadoop', 1)\n",
      "('hdfs', 1)\n",
      "('hadoop', 1)\n",
      "('hdfs', 1)\n",
      "('hadoop', 1)\n",
      "('hdfs', 1)\n",
      "('hive', 1)\n",
      "('spark', 1)\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"C:/Cfamily IT/Data/comment.txt\")\n",
    "print(\"Before Key,Value Pair:\")\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "rdd2 = rdd1.flatMap(lambda x:x.split(\" \"))\n",
    "rdd3 = rdd2.map(lambda x:(x,1))\n",
    "print(\"After Key,Value Pair:\")\n",
    "for row in rdd3.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62102782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c721e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Key,Value Pair:\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "******************************\n",
      "After Key,Value Pair:\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "(1, 10)\n",
      "(2, 10)\n",
      "(3, 10)\n",
      "(4, 10)\n",
      "(5, 10)\n",
      "(6, 10)\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "li = [1,2,3,4,5,6]\n",
    "rdd1 = spark.sparkContext.parallelize(li)\n",
    "print(\"Before Key,Value Pair:\")\n",
    "print(type(rdd1))\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "rdd2 =  rdd1.map(lambda x:(x,10))\n",
    "print(\"After Key,Value Pair:\")\n",
    "print(type(rdd1))\n",
    "for row in rdd2.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0be5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0602d405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Key,Value Pair:\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "******************************\n",
      "After Key,Value Pair:\n",
      "(1, 10)\n",
      "(2, 10)\n",
      "(3, 10)\n",
      "(4, 10)\n",
      "(5, 10)\n",
      "(6, 10)\n",
      "******************************\n",
      "Only Keys:\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "******************************\n",
      "Only Values:\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "******************************\n",
      "Count by Key: defaultdict(<class 'int'>, {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1})\n",
      "******************************\n",
      "Count by Value: defaultdict(<class 'int'>, {(1, 10): 1, (2, 10): 1, (3, 10): 1, (4, 10): 1, (5, 10): 1, (6, 10): 1})\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "li = [1,2,3,4,5,6]\n",
    "rdd1 = spark.sparkContext.parallelize(li)\n",
    "print(\"Before Key,Value Pair:\")\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "rdd2 =  rdd1.map(lambda x:(x,10))\n",
    "print(\"After Key,Value Pair:\")\n",
    "for row in rdd2.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"Only Keys:\")\n",
    "keys = rdd2.keys()\n",
    "for row in keys.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"Only Values:\")\n",
    "values = rdd2.values()\n",
    "for row in values.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"Count by Key:\",rdd2.countByKey())\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"Count by Value:\",rdd2.countByValue())\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf16b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
