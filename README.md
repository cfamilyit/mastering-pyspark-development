# mastering-pyspark-development
How to become Master in PySpark Programming
1.	What is PySpark?
2.	Pre-Requisites for PySpark Programming
3.	Master PySpark Programming Course Content
4.	Who are Target Audience?

What is PySpark?
	It is a Data Processing Framework. It is Purely Data Processing Framework. It does not have any storage. 
	It can take any data and a input like 
Local Filesystem
HDFS Filesystem
Any Cloud Filesystem
Any RDBMS like MySQL, oracle, etc..
Any NoSQL like Apache Cassandra, etc..
Any Streaming Framework like Apache Kafka, etc..
Any Data

And It performs Processing (Distributed Parallel Processing) it uses In-Memory Computing. It is a Fastest Processing Framework.

And write results into any output storage like
Local Filesystem
HDFS Filesystem
Any Cloud Filesystem
Any RDBMS like MySQL, oracle, etc..
Any NoSQL like Apache Cassandra, etc..
Any Streaming Framework like Apache Kafka, etc..
Any Storage
	We have different Spark Components. They are:
PySpark Core
PySpark SQL
PySpark Streaming
PySpark ML/MLib
PySpark GraphX
	We can deploy PySpark Application on
Spark Standalone Cluster
YARN Cluster
Mesos
Kubernetes Cluster

Pre-Requisites:-
1.	Python for Data Engineering – Add on Advantage if you have Core Python + Advanced Python  Completed Python
2.	SQL for Data Engineering – SQL with Any RDBMS
3.	Linux Essentials – (Add on Advantage of Linux+ Shell Scripting)
4.	Any VCS (Version Control System) – Like Git, GitHub, Bit Bucket, etc..
5.	Any Cloud Foundation – Like AWS or Azure or GCP

Master PySpark Programming Course Content
1.	PySpark Core Programming – RDD Programming – Transformations & Actions  Using Python Language.
2.	PySpark SQL – DataFrames, Table, Datasets (Not available in PySpark) – On Data Frames We will Apply DSL (Domain Specific Language) Queries and On table we will apply Native SQL Queries.
E.g:-
empDF.select(“*”)

E.g:-
empDF.registerTempView(“emp”)
spark.sql(“select * from emp”)
3.	PySpark Streaming – Streaming + Live Analytics 
E.g:- Online Shopping
4.	PySpark Integrations
PySpark Integration with Apache Hadoop 
PySpark Integration with Apache Hive
PySpark Integration with Any Cloud Filesystem like AWS S3
PySpark Integration with Any RDBMS like MySQL, Oracle, PostgreSQL, etc..
PySpark Integration with Any NoSQL like Apache Cassandra, MongoDB etc..
PySpark Integration with Any Streaming Frameworks like Apache Kafka, etc..
Etc..






To Become Master in PySpark Developer/Programmer
1.	PySpark Foundation
2.	Python for Data Engineering
3.	PySpark Core Programming – RDD Programming
4.	SQL for Data Engineering
5.	PySpark SQL Programming
6.	AWS Foundation
7.	Linux Essentials
8.	PySpark Cluster Setup (AWS, Java, Scala, Python, MySQL, Apache Hadoop, Apache Hive, Apache Kafka, Apache Cassandra, Apache Spark etc..)
9.	PySpark Integrations
PySpark Integration with Apache Hadoop 
PySpark Integration with Apache Hive
PySpark Integration with Any Cloud Filesystem like AWS S3
PySpark Integration with Any RDBMS like MySQL, Oracle, PostgreSQL, etc..
PySpark Integration with Any NoSQL like Apache Cassandra, MongoDB etc..
PySpark Integration with Any Streaming Frameworks like Apache Kafka, etc..
Etc..
10.	Any VCS (Version Control System) like Git, GitHub, GitLab, Bit Bucket etc..

Who are Target Audience?
1.	Who are Freshers/Experienced – Who Wants to Become Data Engineers
2.	Who are Programmers like Java, Scala, .Net, Python etc..
3.	Who are Database Developer/DBA
4.	Who are Data Warehouse and Reporting People
5.	Non-Programmers like Test Engineers etc..
By
Akkem Sreenivasulu – Founder of CFAMILY IT
Website : www.cfamilyit.com 
eMail: info@cfamilyit.com
Contact/Msg/WhatsApp: +91-9133151144, +91-9133161144
	
